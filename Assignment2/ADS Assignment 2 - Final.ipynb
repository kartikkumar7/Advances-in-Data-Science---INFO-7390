{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The toytext environment chosen for this assignment is <i> Frozen Lake </i> \n",
    "## The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. (This is fixed!)\n",
    "## NOTE: The ice is slippery, so you won't always move in the direction you intend.\n",
    " <img src=\"/ipynb.images/frozen_lake.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will install the necessary libraries and establish a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import gym # loading the Gym library\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 4\n",
      "State: 16\n"
     ]
    }
   ],
   "source": [
    "#Checking the action and state size\n",
    "action_size = env.action_space.n\n",
    "print(\"Action:\", action_size)\n",
    "state_size = env.observation_space.n\n",
    "print(\"State:\", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#initialize q table\n",
    "qtable = np.zeros((state_size,action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters - Baseline Model\n",
    "#Next step: QLearning \n",
    "\n",
    "total_episodes = 5000        # Total episodes\n",
    "# total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 99                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.7           # Learning rate\n",
    "gamma = 0.8                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q Learning \n",
    "\n",
    "rewards_all=[]\n",
    "\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state=env.reset()\n",
    "    step=0\n",
    "    done=False\n",
    "    reward_curr=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:]) #using max Policy\n",
    "            \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action]) #using max policy\n",
    "        \n",
    "        \n",
    "        state = new_state\n",
    "        reward_curr+=reward\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards_all.append(reward_curr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes:\n",
      "1000  :  0.023000000000000013\n",
      "2000  :  0.07300000000000005\n",
      "3000  :  0.1290000000000001\n",
      "4000  :  0.20000000000000015\n",
      "5000  :  0.23900000000000018\n"
     ]
    }
   ],
   "source": [
    "#Calculate and print the average reward per thousand episodes\n",
    "\n",
    "rewards_per_thousand = np.split(np.array(rewards_all), total_episodes/1000)\n",
    "count=1000\n",
    "print(\"Average reward per thousand episodes:\")\n",
    "\n",
    "for r in rewards_per_thousand:\n",
    "    print(count , \" : \" , str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest average rewards = 0.239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #hyperparameters - Trial & Error\n",
    "\n",
    "# total_episodes = 10000        # Total episodes\n",
    "# # total_test_episodes = 100     # Total test episodes\n",
    "# max_steps = 99                # Max steps per episode\n",
    "\n",
    "# learning_rate = 0.9           # Learning rate\n",
    "# gamma = 0.9                 # Discounting rate\n",
    "\n",
    "# # Exploration parameters\n",
    "# epsilon = 1.0                 # Exploration rate\n",
    "# max_epsilon = 1.0             # Exploration probability at start\n",
    "# min_epsilon = 0.001            # Minimum exploration probability \n",
    "# decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters - Manually Optimized Model\n",
    "\n",
    "total_episodes = 10000        # Total episodes\n",
    "# total_test_episodes = 100     # Total test episodes\n",
    "max_steps = 100                # Max steps per episode\n",
    "\n",
    "learning_rate = 0.1           # Learning rate\n",
    "gamma = 0.99                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.001            # Minimum exploration probability \n",
    "decay_rate = 0.001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q Learning \n",
    "\n",
    "rewards_all=[]\n",
    "\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state=env.reset()\n",
    "    step=0\n",
    "    done=False\n",
    "    reward_curr=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:]) #using max Policy\n",
    "                \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action]) #using max policy\n",
    "        \n",
    "        \n",
    "        state = new_state\n",
    "        reward_curr+=reward\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards_all.append(reward_curr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rewards_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes:\n",
      "1000  :  0.05100000000000004\n",
      "2000  :  0.21100000000000016\n",
      "3000  :  0.4150000000000003\n",
      "4000  :  0.5880000000000004\n",
      "5000  :  0.6620000000000005\n",
      "6000  :  0.7070000000000005\n",
      "7000  :  0.7120000000000005\n",
      "8000  :  0.7420000000000005\n",
      "9000  :  0.7470000000000006\n",
      "10000  :  0.7290000000000005\n"
     ]
    }
   ],
   "source": [
    "#Calculate and print the average reward per thousand episodes\n",
    "\n",
    "rewards_per_thousand = np.split(np.array(rewards_all), total_episodes/1000)\n",
    "count=1000\n",
    "print(\"Average reward per thousand episodes:\")\n",
    "\n",
    "for r in rewards_per_thousand:\n",
    "    print(count , \" : \" , str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highest avg reward = 0.747\n",
    "# Which is great considering our baseline\n",
    "\n",
    "Hyperparameter Tuning Observation:\n",
    "--> Increasing number of episodes improved the avg reward somewhat but alone did not make reward high <br>\n",
    "--> Increasing max_steps did not have much effect on the rewards<br>\n",
    "--> Increasing learning rate alone improved the rewards but performance varied when other paramters were changed at the same time<br>\n",
    "--> Reducing gamma reduced the rewards significantly and increasing gamma increased the rewards<br>\n",
    "--> Reducing exploration rate did improve rewards somewhat but after multiple experiments it came to notice that the highest amount a reward could go upto was limited(only around 0.4 something) and nowhere near 1.0<br>\n",
    "--> Increasing decay rate had similar effect as that of exploration rate reduction<br>\n",
    "--> Reducing the minimum exploration probability improved the rewards and increased the time by a little bit for the agent<br>\n",
    "\n",
    "*Note: Only Max Value Policy is used to adjust the hyperparameters<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will change the policy from maximum to random and observe the results\n",
    "Another policy - Minimum Value policy has been commented alongside this code and can be uncommented for experiment.\n",
    "Results for both are mentioned later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q Learning \n",
    "\n",
    "rewards_all=[]\n",
    "\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state=env.reset()\n",
    "    step=0\n",
    "    done=False\n",
    "    reward_curr=0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # 3. Choose an action a in the current world state (s)\n",
    "        ## First we randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "#             action = np.argmax(qtable[state,:]) #using max Policy\n",
    "#             action = np.argmin(qtable[state,:]) #using min Policy\n",
    "            action = np.random.choice(qtable[state,:].size) # using random Policy\n",
    "                \n",
    "        # Else doing a random choice --> exploration\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "#         qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "#                                     np.max(qtable[new_state, :]) - qtable[state, action]) #using max policy\n",
    "#         qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "#                                     np.min(qtable[new_state, :]) - qtable[state, action]) #using max policy\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
    "                                    np.random.choice(qtable[new_state, :]) - qtable[state, action]) #using max policy\n",
    "        \n",
    "        \n",
    "        state = new_state\n",
    "        reward_curr+=reward\n",
    "        \n",
    "        # If done : finish episode\n",
    "        if done == True: \n",
    "            break\n",
    "            \n",
    "    # Reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards_all.append(reward_curr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes:\n",
      "1000  :  0.009000000000000001\n",
      "2000  :  0.008\n",
      "3000  :  0.014000000000000005\n",
      "4000  :  0.013000000000000005\n",
      "5000  :  0.009000000000000001\n",
      "6000  :  0.02000000000000001\n",
      "7000  :  0.016000000000000007\n",
      "8000  :  0.014000000000000005\n",
      "9000  :  0.017000000000000008\n",
      "10000  :  0.016000000000000007\n"
     ]
    }
   ],
   "source": [
    "#Calculate and print the average reward per thousand episodes\n",
    "\n",
    "rewards_per_thousand = np.split(np.array(rewards_all), total_episodes/1000)\n",
    "count=1000\n",
    "print(\"Average reward per thousand episodes:\")\n",
    "\n",
    "for r in rewards_per_thousand:\n",
    "    print(count , \" : \" , str(sum(r/1000)))\n",
    "    count+=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty random as we can see above. \n",
    "# The rewards extremely low which means that we will rarely almost never reach the goal with this policy. We can run the same below and verify this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average steps: 11.35576923076923\n"
     ]
    }
   ],
   "source": [
    "#Run to see agent play FrozenLake (End result of each episode with total number of steps)\n",
    "env.reset()\n",
    "rewards_all =[]\n",
    "total_step=0\n",
    "step_count=0\n",
    "\n",
    "for episode in range(5): # can go upto total_episodes\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    print(\"****Episode\", episode+1, \"****\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if step==max_steps:\n",
    "            print(epsilon)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward==1:\n",
    "                print(\"*** REACHED THE GOAL! *** \", step, \"Steps\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"*** FELL THROUGH A HOLE! ***\", step, \"Steps\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        total_step+=step\n",
    "        step_count+=1\n",
    "    \n",
    "\n",
    "print(\"Average steps:\", total_step/step_count)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above piece of code, we see that Q Learning uses value based iteration, keeping the policy fixed. At a time, we can have one policy only that defines what actions to take based on a state. A policy is a function or a set of rules. \n",
    "\n",
    "\n",
    "The expected lifetime value in the Bellman Equation is the duration for which the agent takes action in the environment.\n",
    "\n",
    "** The total steps almost never reaches the max_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important Citations:\n",
    "https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/\n",
    "https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/\n",
    "https://www.youtube.com/watch?v=Hj4cCmC7jv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Kartik Kumar\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
